{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4 - Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sys, os\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Make', 'Model', 'Year', 'Engine HP', 'Engine Cylinders', 'Transmission Type', 'Vehicle Style',\n",
    "       'highway MPG', 'city mpg', 'MSRP']\n",
    "\n",
    "data = pd.read_csv(r'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv')[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['make', 'model', 'year', 'engine_hp', 'engine_cylinders',\n",
       "       'transmission_type', 'vehicle_style', 'highway_mpg', 'city_mpg',\n",
       "       'msrp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = data.columns.str.lower().str.replace(' ', '_')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make                 False\n",
       "model                False\n",
       "year                 False\n",
       "engine_hp             True\n",
       "engine_cylinders      True\n",
       "transmission_type    False\n",
       "vehicle_style        False\n",
       "highway_mpg          False\n",
       "city_mpg             False\n",
       "msrp                 False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make                 False\n",
       "model                False\n",
       "year                 False\n",
       "engine_hp            False\n",
       "engine_cylinders     False\n",
       "transmission_type    False\n",
       "vehicle_style        False\n",
       "highway_mpg          False\n",
       "city_mpg             False\n",
       "msrp                 False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna(0)\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "msrp_mean = data['msrp'].mean()\n",
    "data['above_average'] = data['msrp'].apply(lambda x: 1 if x > msrp_mean else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.above_average.values \n",
    "X = data.drop(['msrp', 'above_average'], axis=1)\n",
    "\n",
    "X_full_train, X_test, y_full_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_full_train, y_full_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: ROC AUC feature importance\n",
    "\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables.\n",
    "\n",
    "For each numerical variable, use it as score and compute AUC with the above_average variable. Use the training dataset for this task.\n",
    "\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. -df_train['engine_hp'])\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg']\n",
    "\n",
    "auc_dict = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    auc_score = roc_auc_score(y_train, X_train[col].values)\n",
    "    \n",
    "    if auc_score < 0.5:\n",
    "        auc_score = roc_auc_score(y_train, -X_train[col].values)\n",
    "\n",
    "    auc_dict[col] = auc_score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numerical variable that has the highest AUC: engine_hp\n"
     ]
    }
   ],
   "source": [
    "print(f\"The numerical variable that has the highest AUC: {max(auc_dict, key= lambda x: auc_dict[x])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Training the model\n",
    "\n",
    "Apply one-hot-encoding using DictVectorizer and train the logistic regression with these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1_000)\n",
    "dv = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = dv.fit_transform(X_train.to_dict(orient='records'))\n",
    "\n",
    "model.fit(X_train_enc, y_train)\n",
    "\n",
    "X_val_enc = dv.transform(X_val.to_dict(orient='records'))\n",
    "\n",
    "y_pred_val = model.predict_proba(X_val_enc)[:, 1]\n",
    "\n",
    "auc_score = round(roc_auc_score(y_val, y_pred_val), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score is: 0.979\n"
     ]
    }
   ],
   "source": [
    "print(f\"The AUC score is: {auc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Precision and Recall\n",
    "\n",
    "Compute precision and recall for the model.\n",
    "\n",
    "Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "For each threshold, compute precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35000000000000003\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41000000000000003\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47000000000000003\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.5700000000000001\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.6900000000000001\n",
      "0.7000000000000001\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.81\n",
      "0.8200000000000001\n",
      "0.8300000000000001\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.87\n",
      "0.88\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.9400000000000001\n",
      "0.9500000000000001\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.99\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_val == 1)\n",
    "    actual_negative = (y_val == 0)\n",
    "\n",
    "    predict_positive = (y_pred_val >= t)\n",
    "    predict_negative = (y_pred_val < t)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
